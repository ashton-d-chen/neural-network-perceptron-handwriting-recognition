<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><head>

<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="GENERATOR" content="Microsoft FrontPage 4.0">
<meta name="ProgId" content="FrontPage.Editor.Document"><title>Project 4: Classification</title>

<link href="index_files/projects.css" rel="stylesheet" type="text/css"></head><body>
<h2>Project 4: Classification</h2>
<table border="0" cellpadding="10">
<tbody><tr>
<td align="center">
  <img src="index_files/img2.gif"> <br>
  Which Digit?
</td>
<td></td>
<td align="center">
  <table border="0" cellpadding="4">
  <tbody><tr>
    </tr><tr>
      <td><img src="index_files/2002_08_07_img_1578.jpg" width="60"></td>
      <td><img src="index_files/image_0056.jpg" width="60"></td>
      <td><img src="index_files/2002_09_26_img_513.jpg" width="60"></td>
      <td><img src="index_files/image_0067.jpg" width="60"></td>
      <td><img src="index_files/image_0332.jpg" width="60"></td>
    </tr>
    <tr>
      <td><img src="index_files/image_0128.jpg" width="60"></td>
      <td><img src="index_files/2002_10_17_img_647.jpg" width="60"></td>
      <td><img src="index_files/image_0193.jpg" width="60"></td>
      <td><img src="index_files/2002_10_30_img_478.jpg" width="60"></td>
      <td><img src="index_files/2002_10_31_img_507.jpg" width="60"></td>
    </tr>
    <tr>
      <td><img src="index_files/2002_09_05_img_12098.jpg" width="60"></td>
      <td><img src="index_files/2003_02_13_img_642.jpg" width="60"></td>
      <td><img src="index_files/image_0196.jpg" width="60"></td>
      <td><img src="index_files/2003_05_15_img_1197.jpg" width="60"></td>
      <td><img src="index_files/2003_07_04_img_502.jpg" width="60"></td>
    </tr>
  </tbody></table>
  Which are Faces?
</td>
</tr>
</tbody></table>

<br>
<br>
<em>Due 12/06 at 11:59pm</em>
<p><span class="style2"><em>Update: 11/29: Some small fixes to the code. Make sure and download the updated version.</em> 
<h3>Introduction</h3>
<p>In this checkpoint, you will design two classifiers: a naive bayes classifier and a perceptron 
classifier. You will test your classifiers on two image datasets: a set of scanned handwritten digit 
images and a set of face images in which edges have been detected. Even your simple classifiers will 
be able to do quite well on these tasks with enough training data.


</p><p>Optical character recognition (OCR) is the task of extracting text from sources in image formats. The first
set of data you will run your classifiers on is a set of handwritten numerical digits (0-9). This is a very 
commercially useful technology similar to a technique used by the US post office to route mail by zip codes. 
There are systems that can perform with over 99% classification accuracy 
(see <a href="http://yann.lecun.com/exdb/lenet/index.html">LeNet-5</a> for an example system in action).

</p><p>Face detection is the task of localizing faces within video or still images where the faces can be at any 
location and vary in size. There are many applications for face detection including human computer 
interaction and surveillance applications. You will attempt a reduced face detection task in which you are 
presented with an image one which an edge detection algorithm has been computed. Your task will be to determine 
whether the edge image you have been presented is a face or not. There are several systems in use that perform 
quite well at the face detection task. One good system is the <a href="http://www.ri.cmu.edu/projects/project_416.html">Face Detector</a> by Schneiderman and Kanade. You can even try it out on 
your own photos in this <a href="http://demo.pittpatt.com/">demo</a>.


</p><p>The code for this project contains the following files and data files, available as a <a href="http://inst.eecs.berkeley.edu/%7Ecs188/fa06/projects/classification/4/project4.zip">zip file</a>.&nbsp;</p> 
<table border="0" cellpadding="10">
  <tbody><tr>
    <td colspan="2"><b>Classification</b></td>
  </tr>
  <tr>
    <td><a href="http://inst.eecs.berkeley.edu/%7Ecs188/fa06/projects/classification/4/data.zip"><code>data.zip</code></a></td>
    <td>Data file including digit and face data. </td>
  </tr>
  <tr>
    <td><a href="http://inst.eecs.berkeley.edu/%7Ecs188/fa06/projects/classification/4/classificationMethod.py"><code>classificationMethod.py</code></a></td>
    <td>Abstract superclass for the classifiers you will write. You do not need to modify this file. </td>
  </tr>
  <tr>
    <td><a href="http://inst.eecs.berkeley.edu/%7Ecs188/fa06/projects/classification/4/samples.py"><code>samples.py</code></a></td>
    <td>Code to read in the classification data. You do not need to modify this file.  </td>
  </tr>
  <tr>
    <td><a href="http://inst.eecs.berkeley.edu/%7Ecs188/fa06/projects/classification/4/util.py"><code>util.py</code></a></td>
    <td>Code defining some useful tools.  You may be familiar with some of these by now, and 
    they will save you a lot of time.
    </td> </tr>
  <tr>
    <td><a href="http://inst.eecs.berkeley.edu/%7Ecs188/fa06/projects/classification/4/mostFrequent.py"><code>mostFrequent.py</code></a></td>
    <td>A simple example classifier that labels every instance as the most frequent class. You do not 
    need to modify this file</td>
  </tr>
  <tr>
    <td><a href="http://inst.eecs.berkeley.edu/%7Ecs188/fa06/projects/classification/4/naiveBayes.py"><code>naiveBayes.py</code></a></td>
    <td>The main code where you will write your naive bayes classifier. </td>
  </tr>
  <tr>
    <td><a href="http://inst.eecs.berkeley.edu/%7Ecs188/fa06/projects/classification/4/perceptron.py"><code>perceptron.py</code></a></td>
    <td>The main code where you will write your perceptron classifier. </td>
  </tr>
  <tr>
    <td><a href="http://inst.eecs.berkeley.edu/%7Ecs188/fa06/projects/classification/4/dataClassifier.py"><code>dataClassifier.py</code></a></td>
    <td>The wrapper code that will call your classifiers. You will write your enhanced feature extractors here. </td>
  </tr>
</tbody></table>
<p>&nbsp;
</p><p><strong>What to submit:</strong> You will fill in portions of <code>naiveBayes.py</code>,
<code>perceptron.py</code> and <code>dataClassifier.py</code>
(only) during the assignment, and submit them.&nbsp; The only other file you
should submit is a readme in .txt or .pdf format in which you answer any written
questions and in which you should clearly note your partner's name and login (if
you are working in a pair).</p>

<p><strong>Evaluation:</strong> Your code will be autograded for technical
correctness. Please <em>do not</em> change the names of any provided functions
or classes within the code, or you will wreak havoc on the autograder. Your
answers to discussion questions will also be graded.
</p><p><strong>Academic Dishonesty:</strong> We will be checking your code against
other submissions in the class for logical redundancy. If you copy someone
else's code and submit it with minor changes, we will know. These cheat
detectors are quite hard to fool, so please don't try. We trust you all to
submit your own work only; please don't let us down. Instead, contact the course
staff if you are having trouble.

</p><h3>Getting Started</h3>
<p> To try out the classification pipeline, run dataClassifier.py from the command line. This 
will classify the digit data using the default classifier (mostfrequent) which classifies every example
as the most frequent class. Which digit is it picking?

</p><p> &gt; python dataClassifier.py
<xmp> Doing classification
--------------------
No data specified; using digits.
No classifier specified; using default.
No feature extraction function specified; using default.
No training set size specified; using default.
Not doing odds ratio computation.
data:           digits
classifier:             mostfrequent
feature extractor:      basic
training set size:      100
Extracting features...
Training...
Validating...
14 correct out of 100 (14.0%).
Testing...
14 correct out of 100 (14.0%).
&lt;Press enter/return to continue&gt;
</xmp>

</p><h3>Command Line options</h3>
<p>Once you have filled in the appropriate code you can test your classifiers using the following command line options: <br>

&gt; python dataClassifier.py data classifiertype features numtraining <br>
where data is (faces or digits), classifiertype is (naivebayes or perceptron), features is (basic or enhanced),
and numtraining is the number of training examples to use.<br>

</p><p>To run your classifiers with odds ratio computations use:<br>
&gt; python dataClassifier.py data classifiertype features numtraining odds class1 class2<br>
where class1 and class2 are from {0,1,...9} for digits or {0,1} for faces.


</p><p></p><h3>Simple Features</h3>
We have defined some simple features for you to use for your naive Bayes and perceptron classifiers. 
Later you will implement more intelligent features. Our simple features have one feature for
each pixel location that can take values 0 or 1. The features are encoded as a dictionary of 
feature location, value pairs (where location is represented as (column,row) and value is 0 or 1). 

<p>The data for the digit instances are encoded as 28x28 pixel images giving a vector of 784 features for each 
data item. Each feature is set to 0 or 1 depending on whether the pixel is on or not.

</p><p>A canny edge detector has been run on some face and non-face images of size 60x70 pixels, giving a vector
of 4200 features for each item. Like the digits, these features can take values 0 or 1 depending on whether 
there was an edge detected at each pixel.


</p><h2>1.) Naive Bayes</h2>
A skeleton implementation of a naive Bayes classifier is provided for you in naiveBayes.py. You will fill in the 
train function, the calculatePosteriorProbabilities function and the findHighOddsFeatures function.

<p>A naive Bayes classifier
 models a joint distribution over a label <img src="index_files/img1.png" alt="$Y$" align="bottom" border="0" height="14" width="17"> and a set of observed
 random variables, or <i>features</i>, <!-- MATH
 $\{F_1, F_2, \ldots F_n\}$
 -->
<img src="index_files/img2.png" alt="$\{F_1, F_2, \ldots F_n\}$" align="middle" border="0" height="32" width="109">,
 using the assumption that the full joint distribution can be factored
 as follows:
 <br></p><p></p>
<div align="center">
<!-- MATH
 \begin{displaymath}
P(F_1 \ldots F_n, Y) = P(Y) \prod_i P(F_i | Y)
\end{displaymath}
 -->
<img src="index_files/img3.png" alt="\begin{displaymath}
P(F_1 \ldots F_n, Y) = P(Y) \prod_i P(F_i \vert Y)
\end{displaymath}" border="0" height="44" width="246">
</div>
<br clear="all">
<p></p>

<p>
To classify a datum, we can find the most probable class given the feature values for each pixel:
</p><p></p>
<div align="center">
<!-- MATH
 \begin{eqnarray*}
P(y\ |\ f_1, \ldots, f_m) &=& \frac{P(f_1, \ldots, f_m\ |\ y) P(y)}{P(f_1, \ldots, f_m)} \\
  &=& \frac{P(y) \prod_{i = 1}^m P(f_i\ |\ y) }{P(f_1, \ldots, f_m)} \\
  \textmd{arg max}_{y} P(y\ |\ f_1, \ldots, f_m) &=& \textmd{arg max}_{y} \frac{P(y) \prod_{i = 1}^m P(f_i\ |\ y) }{P(f_1, \ldots, f_m)} \\
  &=& \textmd{arg max}_{y} P(y) \prod_{i = 1}^m P(f_i\ |\ y)
\end{eqnarray*}
 -->
<img src="index_files/img4_new.png" alt="\begin{eqnarray*}
P(y \vert f_1, \ldots, f_m) &amp;=&amp; \frac{P(f_1, \ldots, f_m \...
...
&amp;=&amp; \textmd{arg max}_{y} P(y) \prod_{i = 1}^m P(f_i \vert y)
\end{eqnarray*}" border="0" height="169" width="491"></div>
<br clear="all"><p></p>

<p>
Because multiplying many probabilities together often results in underflow, we will instead compute log
probability which will have the same argmax:
</p><p></p>
<div align="center">
<!-- MATH
 \begin{eqnarray*}
\textmd{arg max}_{y} log(P(y\ |\ f_1, \ldots, f_m) &=& \textmd{arg max}_{y} (log(P(y)) + \sum_{i = 1}^m log(P(f_i\ |\ y)))
\end{eqnarray*}
 -->
<img src="index_files/img5.png" alt="\begin{eqnarray*}
\textmd{arg max}_{y} log(P(y \vert f_1, \ldots, f_m) &amp;=&amp; \te...
...{arg max}_{y} (log(P(y)) + \sum_{i = 1}^m log(P(f_i \vert y)))
\end{eqnarray*}" border="0" height="53" width="535"></div>
<br clear="all"><p></p>

<p>
</p><h4>1.1) Parameter Estimation</h4>
Our naive Bayes model has several parameters to estimate.  One
parameter is the prior distribution over labels (digits, or face/not-face),
<img src="index_files/img6.png" alt="$P(Y)$" align="middle" border="0" height="32" width="42">.

<p>
We can estimate <img src="index_files/img6.png" alt="$P(Y)$" align="middle" border="0" height="32" width="42"> directly from the training data:
<br></p><p></p>
<div align="center">
<!-- MATH
 \begin{displaymath}
\hat{P}(y) = \frac{c(y)}{n}
\end{displaymath}
 -->
<img src="index_files/img7.png" alt="\begin{displaymath}
\hat{P}(y) = \frac{c(y)}{n}
\end{displaymath}" border="0" height="40" width="84">
</div>
<br clear="all">
<p></p>
where <img src="index_files/img8.png" alt="$c(y)$" align="middle" border="0" height="32" width="32"> is the number of training instances with label y and
n is the total number of training instances.

<p>
The other parameters to estimate are the conditional probabilities of
our features given each label y: <!-- MATH
 $P(F_i | Y = y)$
 -->
<img src="index_files/img9.png" alt="$P(F_i \vert Y = y)$" align="middle" border="0" height="32" width="92">. We do this for each
possible feature value (<img src="index_files/img10.png" alt="$f_i \in {0,1}$" align="middle" border="0" height="30" width="60">).
</p><p></p>
<div align="center">
<!-- MATH
 \begin{eqnarray*}
\hat{P}(F_i=f_i|Y=y) &=& \frac{c(f_i,y)}{\sum_{f_i}{c(f_i,y)}} \\
\end{eqnarray*}
 -->
<a name="empirical"></a><img src="index_files/img11.png" alt="\begin{eqnarray*}
\hat{P}(F_i=f_i\vert Y=y) &amp;=&amp; \frac{c(f_i,y)}{\sum_{f_i}{c(f_i,y)}} \\
\end{eqnarray*}" border="0" height="53" width="248"></div>
<br clear="all"><p></p>
where <img src="index_files/img12.png" alt="$c(f_i,y)$" align="middle" border="0" height="32" width="52"> is the number of times pixel <img src="index_files/img13.png" alt="$F_i$" align="middle" border="0" height="30" width="20"> took value <img src="index_files/img14.png" alt="$f_i$" align="middle" border="0" height="30" width="18">
in the training examples of class y.

</span></p><h4>1.2) Smoothing</h4>
Your current parameter estimates are <i>unsmoothed</i>, that is, you are
using the empirical estimates for the parameters <img src="index_files/img15.png" alt="$P(f_i\vert y)$" align="middle" border="0" height="32" width="55">.  These
estimates are rarely adequate in real systems.  Minimally, we need to
make sure that no parameter ever receives an estimate of zero, but
good smoothing can boost accuracy quite a bit by reducing
overfitting.

<p>
The basic smoothing method we'll use here is <i>Laplace Smoothing</i> which 
essentially adds k counts to every possible observation value:
</p><p>
<!-- MATH
 $P(F_i=f_i|Y=y) = \frac{c(F_i=f_i,Y=y)+k}{\sum_{f_i}{c(F_i=f_i,Y=y)+k}}$
 -->
<img src="index_files/imgsmoothlaplace.png" alt="$P(F_i=f_i\vert Y=y) = \frac{c(F_i=f_i,Y=y)+k}{\sum_{f_i}{c(F_i=f_i,Y=y)+k}}$" align="middle" border="0" height="40" width="275">

</p><p>
If k=0 the probabilities are unsmoothed, as k grows larger the probabilities are 
smoothed more and more. You can use your validation set to determine a good value 
for k (note: you don't have to smooth P(C)).

</p><ul>
<li>Test your Naive Bayes classifier on both the digit data and the face data with 100 training samples for k=1. 
What are your classification accuracies?
</li>
<li>Plot the validation set accuracy on digits vs k using 100 and 1000 training examples.
Approximately what value of k is optimal for your classifier for each of these training set sizes?  
Can you explain the difference?
</li>
<li>Plot the validation set accuracy for digits by the size of the training set up to 2500 training examples.  
What can you observe about the performance?  Does it look like it's leveled off?

</li>
</ul>

<h4>1.3) Odds Ratios</h4>
One important skill in using classifiers in real domains is being able
to inspect what they have learned.  One way to inspect a naive Bayes
model is to look at the most likely features for a given class.

<p>
Another tool for understanding the parameters is to look at <i>odds ratios</i>.  For each pixel
feature <img src="index_files/img13.png" alt="$F_i$" align="middle" border="0" height="30" width="20"> and classes <img src="index_files/img23_new.png" alt="$y_1, y_2$" align="middle" border="0" height="30" width="41">, consider the odds ratio:
<br></p><p></p>
<div align="center">
<!-- MATH
 \begin{displaymath}
\mbox{odds}(F_i=on, y_1, y_2) = \frac{P(F_i=on|y_1)}{P(F_i=on|y_2)}
\end{displaymath}
 -->
<img src="index_files/img24.png" alt="\begin{displaymath}
\mbox{odds}(F_i=on, y_1, y_2) = \frac{P(F_i=on\vert y_1)}{P(F_i=on\vert y_2)}
\end{displaymath}" border="0" height="44" width="263">
</div>
<br clear="all">
<p></p>
This ratio will be greater than one for features which cause belief in
<img src="index_files/img25_new.png" alt="$y_1$" align="middle" border="0" height="30" width="19"> to increase relative to <img src="index_files/img26_new.png" alt="$y_2$" align="middle" border="0" height="30" width="19">.

<p>
The features that will have the greatest impact at classification time are those with both a high
probability (because they appear often in the data) and a high odds ratio (because they strongly bias
one label versus another).

</p><p>
Fill in the function findHighOddsFeatures(self, class1, class2). It should return 3 counters, featuresClass1
which are the 100 features with largest <!-- MATH
 $P(F_i=on|class1)$
 -->
<img src="index_files/img27.png" alt="$P(F_i=on\vert class1)$" align="middle" border="0" height="32" width="131">, featuresClass2 which are the 100 features
with largest <!-- MATH
 $P(F_i=on|class2)$
 -->
<img src="index_files/img28.png" alt="$P(F_i=on\vert class2)$" align="middle" border="0" height="32" width="131">, and  featuresOdds the 100 features with highest odds ratios for class1
over class2.

</p><p>

</p><ul>
<li>Display the 100 most likely pixels for the numbers <img src="index_files/img29.png" alt="$3$" align="bottom" border="0" height="13" width="12"> and <img src="index_files/img30.png" alt="$6$" align="bottom" border="0" height="13" width="12"> as well as the pixels with the highest
odds ratios.
</li>
<li>Plot the most likely pixels for the face and non-face classes as well as the pixels with the highest
odds ratios.
</li>
<li>Why do these plots look like they do? Print out your plots and turn them in with your write-up.

</li>
</ul>


<h2>2.) Perceptron</h2>
A skeleton implementation of a perceptron classifier is provided for you in perceptron.py. You will fill in the 
train function, and the findHighOddsFeatures function.

<p>
Unlike the naive Bayes classifier, the perceptron does not use
probabilities to make its decisions.  Instead, it keeps a
<i>prototype</i> <img src="index_files/img31_new.png" alt="$w^y$" align="bottom" border="0" height="17" width="24"> of each class <img src="index_files/img32_new.png" alt="$y$" align="middle" border="0" height="30" width="13">.  Given a feature list <img src="index_files/img33.png" alt="$f$" align="middle" border="0" height="30" width="14">,
the perceptron predicts the class <img src="index_files/img32_new.png" alt="$y$" align="middle" border="0" height="30" width="13"> whose prototype is most similar
to the input vector <img src="index_files/img33.png" alt="$f$" align="middle" border="0" height="30" width="14">.  Formally, given a feature vector <img src="index_files/img33.png" alt="$f$" align="middle" border="0" height="30" width="14"> (a map
from properties to counts, pixels to intensities), we score each class with:
<br></p><p></p>
<div align="center">
<!-- MATH
 \begin{displaymath}
\mbox{score}(f,y) = \sum_i f_i w^y_i
\end{displaymath}
 -->

<img src="index_files/img34.png" alt="\begin{displaymath}
\mbox{score}(f,y) = \sum_i f_i w^y_i
\end{displaymath}" border="0" height="44" width="149">
</div>
<br clear="all">
<p></p>
Then we pick the class with highest score as the label for that datum.
In the code, we will represent <img src="index_files/img31_new.png" alt="$w^y$" align="bottom" border="0" height="17" width="24"> as a Counter, which maps features
(pixels) to their count in digit <img src="index_files/img32_new.png" alt="$y$" align="middle" border="0" height="30" width="13">'s prototype.

<h4>2.1) Learning weights</h4>
ll we need is a method of learning the prototype weights.  In the
basic multi-class perceptron, we scan over the data, one instance at a
time.  When we come to an instance <img src="index_files/img35.png" alt="$(f, y)$" align="middle" border="0" height="32" width="41">, we calculate the <i>model prediction</i>:
<br><p></p>
<div align="center">
<!-- MATH
 \begin{displaymath}
y' = \textmd{arg max}_{y''} score(f,y'')
\end{displaymath}
 -->
<img src="index_files/img36.png" alt="\begin{displaymath}
y' = \textmd{arg max}_{y''} score(f,y'')
\end{displaymath}" border="0" height="30" width="187">
</div>
<br clear="all">
<p></p>
We compare <img src="index_files/img37.png" alt="$y'$" align="middle" border="0" height="32" width="17"> to the true label <img src="index_files/img32_new.png" alt="$y$" align="middle" border="0" height="30" width="13">.  If <img src="index_files/img38.png" alt="$y' = y$" align="middle" border="0" height="32" width="47">, we've gotten the
instance correct, and we do nothing.  Otherwise, we guessed <img src="index_files/img37.png" alt="$y'$" align="middle" border="0" height="32" width="17"> but
we should have guessed <img src="index_files/img32_new.png" alt="$y$" align="middle" border="0" height="30" width="13">.  That means that the prototype <img src="index_files/img31_new.png" alt="$w^y$" align="bottom" border="0" height="17" width="24"> needs
to be more like <img src="index_files/img33.png" alt="$f$" align="middle" border="0" height="30" width="14"> and the prototype <img src="index_files/img39.png" alt="$w^{y'}$" align="bottom" border="0" height="18" width="28"> needs to be less like
<img src="index_files/img33.png" alt="$f$" align="middle" border="0" height="30" width="14"> to help prevent this error in the future.  We update these two prototypes:
<br><p></p>
<div align="center">
<!-- MATH
 \begin{displaymath}
w^y += f
\end{displaymath}
 -->
<img src="index_files/img40.png" alt="\begin{displaymath}
w^y += f
\end{displaymath}" border="0" height="27" width="62">
</div>
<br clear="all">
<div align="center">
<!-- MATH
 \begin{displaymath}
w^{y'} -= f
\end{displaymath}
 -->

<img src="index_files/img41.png" alt="\begin{displaymath}
w^{y'} -= f
\end{displaymath}" border="0" height="27" width="67">
</div>
<br clear="all">
<p></p>

<p>
Using the adding, subtracting, and multiplying functionality of the
<tt>Counter</tt> class in <b>util.py</b>, the perceptron updates should be
relatively easy to code.  Certain implementation issues have been
taken care of for you in <b>perceptron.py</b>, such as handling iterations
over the training data and ordering the update trials.  Furthermore,
the code sets up the <tt>weights</tt> data structure for you.  Each
legal label needs its own protoype <tt>Counter</tt> full of weights.

</p><p>

</p><ul>
<li>Write the  <tt>train</tt> method for the perceptron algorithm and test it using the basic
  pixel features on the face and digit data. What classification performance do you get for each?
</li>
<li>An important factor in getting good performance out of an iteratively trained algorithm like the perceptron is not <i>overtraining</i>.
For now, you code stops after 5 iterations. Can you improve accuracy by
using the held-out data to decide when to stop training.
</li>
</ul>


<h4>2.2) Visualizing weights</h4>Perceptron classifiers and other
discriminative methods are often criticized because the parameters they
learn are hard to interpret. To see a demonstration of this issue, we
can repeat the visualization exercise from the naive Bayes classifier.
<p>
Fill in the function findHighOddsFeatures(self, class1, class2). It should return 3 counters, featuresClass1
which are the 100 features with largest weights for class1, featuresClass2 which are the 100 features
with largest weights for class2, and  featuresOdds the 100 features with highest difference in feature
weights.

</p><p>

</p><ul>
<li>Display the 100 pixels with the largest weights for the numbers <img src="index_files/img29.png" alt="$3$" align="bottom" border="0" height="13" width="12"> and <img src="index_files/img30.png" alt="$6$" align="bottom" border="0" height="13" width="12"> and for difference in
  weights <img src="index_files/img42.png" alt="$w^3 - w^6$" align="middle" border="0" height="34" width="62">. 
</li>
<li>Display the 100 pixels with the largest weights for face and non-face classes and for difference in
  weights <!-- MATH
 $w^{face} - w^{non-face}$
 -->
<img src="index_files/img43.png" alt="$w^{face} - w^{non-face}$" align="middle" border="0" height="35" width="133">. 
</li>
<li>Include these images in your write-up. Comment on how these plots
differ from the equivalent plots of the naive Bayes classifier. Can you
interpret them?
</li>
</ul>


<h2>3.) Feature Design</h2>
<p>Building classifiers is only a small part of getting a good system working for a task.  Indeed, the 
main difference between a good system and a bad one is usually not the classifier itself (e.g. perceptron vs.
naive Bayes), but rather rests on the quality of the features used.  So far, we have used the simplest 
possible features: the identity of each pixel.

</p><p>To increase your classifier's accuracy further, you will need to extract more useful features from 
the data.  The EnhancedFeatureExtractorDigit and EnhancedFeatureExtractorFace functions in dataClassifier.py are 
your new playground.  Look at some of your errors.  You should look for characteristics of the input that would 
give the classifier useful information about the label.  For instance in the digit data, consider the number of 
separate, connected regions of white pixels, which varies by digit type.  1, 2, 3, 5, 7 tend to have one 
contiguous region of white space while the loops in 6, 8, 9 create more.  The number of white regions in a 
4 depends on the writer.  This is an example of a feature that is not directly available to the classifier 
from the per-pixel information.  If your feature extractor adds new features that encode these properties, 
the classifier will be able exploit them.

</p><p>Try adding some other features for both the digit and face datasets and see how they affect performance.  
Before implementing features haphazardly, think about what additional information would help guide your 
classifier on its mistakes.  This question will not be graded on improvement, but instead on the thoughtfulness 
by which you consider your classifier's errors and attempt to capture new information that might remedy them. 
Also test to see if certain features help performance when the classifier has only a few training instances.
You should implement at least one new feature type of your devising for each dataset, but you should discuss at 
least 3 ideas total.

</p><p>
</p><h2>Tests</h2>
We will run the following tests on new test data. So, make sure your submitted code runs correctly for 
the following commands.

<xmp>
python dataClassifier.py digits naivebayes basic 1000
python dataClassifier.py faces naivebayes basic 100
python dataClassifier.py digits naivebayes enhanced 1000
python dataClassifier.py faces naivebayes enhanced 100

python dataClassifier.py digits perceptron basic 1000
python dataClassifier.py faces perceptron basic 100
python dataClassifier.py digits perceptron enhanced 1000
python dataClassifier.py faces perceptron enhanced 100
</xmp>
</body></html>